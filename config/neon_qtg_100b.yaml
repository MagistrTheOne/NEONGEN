# NEON-QTG-100B Model Configuration - RESEARCH BREAKTHROUGH
# Realistic 100B parameter model for 2026-2027
# Research-ready distributed training configuration

metadata:
  name: "NEON-QTG-100B"
  tagline: "RESEARCH BREAKTHROUGH"
  parameters: "100B"
  cost: "$2M"
  gpu_required: "8×H200"
  status: "RESEARCH READY"
  timeline: "2026-2027"

model:
  # Architecture parameters (realistic for 100B model)
  vocab_size: 30000
  embed_dim: 8192      # 16x increase from 512
  num_layers: 80       # 6.7x increase from 12
  num_heads: 64        # 8x increase from 8
  max_seq_len: 8192    # 16x increase from 512

  # QTG-specific parameters
  persistence_threshold: 0.1
  tau: 1.0
  gamma: 1.0
  dropout: 0.1

# Memory optimization settings - realistic for 8×H200
memory:
  # Training parameters (realistic for 100B model)
  batch_size: 2          # per GPU
  gradient_accumulation_steps: 16  # effective batch size 32
  effective_batch_size: 32

  # Mixed precision
  fp16: true
  fp16_opt_level: "O1"

  # Memory management
  empty_cache_steps: 10
  max_grad_norm: 1.0

  # Sequence optimization
  max_seq_len: 8192
  gradient_checkpointing: true

  # REALISTIC OPTIMIZATIONS for 100B scale
  activation_checkpointing: true
  sparse_attention: true        # reduce quadratic complexity
  attention_window: 2048        # reasonable window size
  multi_query_attention: true   # reduce KV heads for efficiency
  kv_heads: 8                   # 8 KV heads instead of 64
  flash_attention: true         # H200 optimized attention
  distributed_training: true    # 8 GPUs required
  pipeline_parallelism: true    # tensor/model parallelism
  zero_optimization: true       # memory-efficient training

# Training parameters - realistic for 100B model
training:
  # Optimizer (realistic for 100B model)
  optimizer: "adamw"
  lr: 2.0e-5        # reasonable learning rate
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999

  # Learning rate scheduler
  scheduler: "cosine"
  warmup_steps: 10000   # reasonable warmup
  total_steps: 1000000  # 1M steps for convergence

  # Loss weights
  lambda_topo: 0.1
  lambda_geo: 0.05
  lambda_quantum: 0.01

# Data parameters - realistic for 100B model
data:
  # Dataset (realistic for 100B model)
  dataset_name: "openwebtext+c4+pile"
  max_samples: 100000000  # 100M samples

  # Preprocessing (realistic for 8K sequences)
  max_length: 8192        # match model max_seq_len
  stride: 4096            # reasonable stride
  batch_size: 2           # per GPU processing

# Evaluation parameters - realistic for 100B model
evaluation:
  # Metrics (realistic for 100B model)
  eval_steps: 10000     # reasonable evaluation frequency
  save_steps: 50000     # reasonable checkpoint frequency
  eval_batch_size: 2    # per GPU eval

  # Generation (realistic for 100B model)
  max_gen_length: 4096  # half of context
  temperature: 0.8
  top_p: 0.9
  top_k: 50

# Logging and monitoring
logging:
  log_steps: 10
  eval_log_steps: 100
  save_total_limit: 3
  load_best_model_at_end: true

# Hardware-specific optimizations - realistic for 8×H200
hardware:
  gpu_memory_gb: 200      # H200 has 200GB VRAM per GPU
  cpu_cores: 64           # reasonable CPU cluster
  use_pinned_memory: true
  num_workers: 16         # reasonable data loading
  prefetch_factor: 4      # reasonable prefetching

  # REALISTIC CLUSTER OPTIMIZATIONS
  use_bfloat16: true      # better precision for large models
  enable_tensor_cores: true  # H200 tensor cores
  multi_node_training: true   # 8 nodes required
  cluster_size: 8         # 8 H200 GPUs
  network_bandwidth: "100Gbps"  # reasonable interconnect
  storage_capacity: "10TB"      # reasonable storage for checkpoints
