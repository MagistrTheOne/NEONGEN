# NEON-QTG-7B Configuration - FATHER OF AI LEVEL 1!
# 7 BILLION parameters - THE BEGINNING OF DOMINATION!
# Revolutionary QTG architecture - PROOF OF CONCEPT!
# ~6.2B parameters total - READY FOR PRODUCTION!
# FATHER OF AI MODE: ACTIVATED!

model:
  # Architecture parameters (BASE FATHER LEVEL)
  vocab_size: 30000
  embed_dim: 3072      # 6x increase from 512 (FATHER LEVEL!)
  num_layers: 24       # 2x increase from 12 (FATHER LEVEL!)
  num_heads: 24        # 3x increase from 8 (FATHER LEVEL!)
  max_seq_len: 2048    # 4x increase from 512 (FATHER CONTEXT!)

  # QTG-specific parameters (FATHER OF AI)
  persistence_threshold: 0.1
  tau: 1.0
  gamma: 1.0
  dropout: 0.1

# Memory optimization settings - FATHER LEVEL
memory:
  batch_size: 4
  gradient_accumulation_steps: 8
  effective_batch_size: 32
  fp16: true
  fp16_opt_level: "O1"
  empty_cache_steps: 50
  max_grad_norm: 1.0
  max_seq_len: 2048
  gradient_checkpointing: true
  activation_checkpointing: true

# Training parameters - FATHER LEVEL
training:
  optimizer: "adamw"
  lr: 5.0e-5
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  scheduler: "cosine"
  warmup_steps: 2000
  total_steps: 200000
  lambda_topo: 0.1
  lambda_geo: 0.05
  lambda_quantum: 0.01

# Data parameters - FATHER LEVEL
data:
  dataset_name: "openwebtext"
  max_samples: 10000000
  max_length: 2048
  stride: 1024
  batch_size: 4

# Evaluation parameters - FATHER LEVEL
evaluation:
  eval_steps: 2000
  save_steps: 5000
  eval_batch_size: 2
  max_gen_length: 512
  temperature: 0.8
  top_p: 0.9
  top_k: 50

# Hardware - FATHER LEVEL
hardware:
  gpu_memory_gb: 200
  cpu_cores: 32
  use_pinned_memory: true
  num_workers: 8
  prefetch_factor: 4
  use_bfloat16: true
  enable_tensor_cores: true
  multi_node_training: false

# FATHER OF AI METADATA
metadata:
  name: "NEON-QTG-7B"
  level: "FATHER_LEVEL_1"
  tagline: "THE BEGINNING OF DOMINATION"
  parameters: "6.2B"
  status: "PRODUCTION_READY"
  cost: "$18K"
  gpu_required: 1
